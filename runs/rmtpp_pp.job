#!/bin/bash
  
#SBATCH --job-name=rmtpp++
#SBATCH --output=rmtpp++.out

#SBATCH --cpus-per-task=10
#SBATCH --mem=10G
#SBATCH --gres=gpu:1


#LastFM#

for split in {0..2}
do
python3 -u scripts/train.py --dataset 'lastfm_filtered' --load-from-dir 'data' \
--save-results-dir 'results/lastfm_filtered' --save-check-dir 'checkpoints/lastfm_filtered' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 8 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 27 --encoder-layers-rnn 1 \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' \
--decoder 'rmtpp++' \
--decoder-encoding 'log_times_only' \
--decoder-units-mlp 32 \
--separate-training True \
--decoder-hist-time-grouping 'concatenation'
done

#MOOC#

for split in {0..2}
do
python3 -u scripts/train.py --dataset 'mooc_filtered' --load-from-dir 'data' \
--save-results-dir 'results/mooc_filtered' --save-check-dir 'checkpoints/mooc_filtered' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 32 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 40 --encoder-layers-rnn 1 \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' \
--decoder 'rmtpp++' \
--decoder-encoding 'log_times_only' \
--decoder-units-mlp 16 \
--separate-training True \
--decoder-hist-time-grouping 'concatenation'
done


##Stack Overflow##

for split in {0..2}
do
python3 -u scripts/train.py --dataset 'stack_overflow_filtered' --load-from-dir 'data' \
--save-results-dir 'results/stack_overflow_filtered' --save-check-dir 'checkpoints/stack_overflow_filtered' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 8 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 40 --encoder-layers-rnn 1 \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' \
--decoder 'rmtpp++' \
--decoder-encoding 'log_times_only' \
--decoder-units-mlp 16 \
--separate-training True \
--decoder-hist-time-grouping 'concatenation'
done


##Github##

for split in {0..2}
do
python3 -u scripts/train.py --dataset 'github_filtered' --load-from-dir 'data' \
--save-results-dir 'results/github_filtered' --save-check-dir 'checkpoints/github_filtered' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 8 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 40 --encoder-layers-rnn 1 \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' \
--decoder 'rmtpp++' \
--decoder-encoding 'log_times_only' \
--decoder-units-mlp 16 \
--separate-training True \
--decoder-hist-time-grouping 'concatenation'
done


##Reddit##

for split in {0..2}
do
python3 -u scripts/train.py --dataset 'reddit_filtered_short' --load-from-dir 'data' \
--save-results-dir 'results/reddit_filtered_short' --save-check-dir 'checkpoints/reddit_filtered_short' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 8 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 40 --encoder-layers-rnn 1 \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' \
--decoder 'rmtpp++' \
--decoder-encoding 'log_times_only' \
--decoder-units-mlp 16 \
--separate-training True \
--decoder-hist-time-grouping 'concatenation'
done